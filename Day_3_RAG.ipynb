{ 
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nareshedagotti/RAG/blob/main/Day_3_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Day 3: Embeddings – The Heart of RAG Systems**\n"
      ],
      "metadata": {
        "id": "fW9-VhZLLUkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **What Are Embeddings and Why They Matter in RAG Systems?**\n",
        "\n",
        "In our journey through Retrieval Augmented Generation (RAG), we've explored the fundamentals and document processing. Today, we dive into arguably the most important component: embeddings. These mathematical representations power the \"retrieval\" in RAG and determine whether your system finds relevant information or misses the mark entirely.\n",
        "\n",
        "##### **What Are Embeddings?**\n",
        "\n",
        "Embeddings are numerical representations of text (or other data) in the form of dense vectors—essentially lists of numbers. These vectors capture the semantic meaning of the text in a way that computers can understand and compare.\n",
        "Think of embeddings as translating human language into \"computer language.\" When we convert words, sentences, or documents into embeddings, we're creating mathematical representations that preserve semantic relationships.\n",
        "\n",
        "##### **The Mathematics Behind Embeddings**\n",
        "\n",
        "At their core, embeddings map words or text to points in a high-dimensional space (typically hundreds or thousands of dimensions). Let's break this down with a simple example:\n",
        "\n",
        "Imagine we want to represent three simple words with 3-dimensional embeddings (real embeddings use many more dimensions):\n",
        "\n",
        "\"King\" = [0.7, 0.2, 0.1]\n",
        "\n",
        "\"Queen\" = [0.6, 0.3, 0.1]\n",
        "\n",
        "\"Apple\" = [0.1, 0.2, 0.8]\n",
        "\n",
        "Notice how \"King\" and \"Queen\" have similar vectors because they're semantically related (both royalty), while \"Apple\" has a very different vector (it's an unrelated concept).\n",
        "\n",
        "In practice, embedding models like those from OpenAI or HuggingFace perform complex calculations using neural networks to produce these vectors. The calculations involve:\n",
        "1. Breaking text into tokens (words or subwords)\n",
        "2. Passing these tokens through layers of a neural network\n",
        "3. Extracting the final layer's numerical representations\n",
        "\n",
        "The exact computation involves matrix multiplications, non-linear activation functions, and other operations that capture patterns from massive training datasets of text."
      ],
      "metadata": {
        "id": "IVqIj-dALtCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Why Use Embeddings?**\n",
        "\n",
        "Embeddings solve a fundamental problem: computers don't naturally understand human language or concepts. By converting text to vectors, we can:\n",
        "\n",
        "1. Measure semantic similarity between texts mathematically\n",
        "2. Search by meaning rather than just keywords\n",
        "3. Cluster similar concepts together\n",
        "4. Find relevant information even when exact words don't match\n",
        "\n",
        "##### **Real-world Example**\n",
        "\n",
        "Imagine a user asks: \"What's the treatment for high blood pressure?\"\n",
        "\n",
        "* **Without embeddings:** A simple keyword search might miss documents that talk about \"hypertension treatment\" or \"reducing elevated blood pressure\" because the exact words don't match.\n",
        "* **With embeddings:** The system understands that \"high blood pressure\" and \"hypertension\" are semantically similar and retrieves relevant documents regardless of the exact terminology used."
      ],
      "metadata": {
        "id": "r_CHepMoN88I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Measuring Semantic Similarity**\n",
        "\n",
        "The power of embeddings comes from our ability to mathematically measure how similar two pieces of text are. The two most common similarity metrics are:\n",
        "\n",
        "1. Cosine Similarity\n",
        "\n",
        "Cosine similarity measures the cosine of the angle between two vectors. It ranges from -1 (completely opposite) to 1 (identical).\n",
        "\n",
        "For two embedding vectors A and B, the cosine similarity is calculated as:\n",
        "\n",
        "Cosine Similarity = (A · B) / (||A|| × ||B||)\n",
        "\n",
        "Where:\n",
        "\n",
        "* A · B is the dot product: sum(A[i] × B[i]) for all dimensions i\n",
        "* ||A|| is the magnitude (length) of vector A: sqrt(sum(A[i]²))\n",
        "* ||B|| is the magnitude (length) of vector B: sqrt(sum(B[i]²))\n",
        "\n",
        "**Example calculation** with our simplified vectors:\n",
        "\n",
        "* A = \"King\" = [0.7, 0.2, 0.1]\n",
        "* B = \"Queen\" = [0.6, 0.3, 0.1]\n",
        "\n",
        "Dot product: (0.7 × 0.6) + (0.2 × 0.3) + (0.1 × 0.1) = 0.42 + 0.06 + 0.01 = 0.49\n",
        "\n",
        "||A|| = sqrt(0.7² + 0.2² + 0.1²) = sqrt(0.49 + 0.04 + 0.01) = sqrt(0.54) ≈ 0.735\n",
        "\n",
        "||B|| = sqrt(0.6² + 0.3² + 0.1²) = sqrt(0.36 + 0.09 + 0.01) = sqrt(0.46) ≈ 0.678\n",
        "\n",
        "Cosine Similarity = 0.49 / (0.735 × 0.678) ≈ 0.49 / 0.498 ≈ 0.984\n",
        "\n",
        "This high value (close to 1) indicates \"King\" and \"Queen\" are very similar in our embedding space.\n"
      ],
      "metadata": {
        "id": "gReBI694Okou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2. Euclidean Distance**\n",
        "\n",
        "Euclidean distance measures the straight-line distance between two points in space. Lower values indicate similarity.\n",
        "\n",
        "Euclidean Distance = sqrt(sum((A[i] - B[i])²))\n",
        "\n",
        "For our example:\n",
        "\n",
        "* √((0.7-0.6)² + (0.2-0.3)² + (0.1-0.1)²)\n",
        "* √(0.01 + 0.01 + 0)\n",
        "* √0.02 ≈ 0.141\n",
        "\n",
        "This relatively small distance also indicates similarity between \"King\" and \"Queen\".\n",
        "\n",
        "**Why cosine similarity is preferred:** Cosine similarity focuses on the direction of vectors rather than magnitude, making it less sensitive to document length and more focused on content similarity.\n"
      ],
      "metadata": {
        "id": "zQNVoiMEPeAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Types of Embeddings**\n",
        "\n",
        "Not all embeddings are created equal. Here are the main types:\n",
        "\n",
        "1. Word Embeddings\n",
        "\n",
        " * What they are: Represent individual words as vectors\n",
        " * Examples: Word2Vec, GloVe, FastText\n",
        " * Limitations: Don't capture context well; \"bank\" (financial) and \"bank\" (river) have the same embedding\n",
        "\n",
        " Word2Vec(\"bank\") = [0.2, 0.5, -0.1, ...]\n",
        "2. Contextual Embeddings\n",
        "\n",
        " * What they are: Represent words based on surrounding context\n",
        " * Examples: BERT, RoBERTa, T5\n",
        " * Advantages: Capture word meaning in context; \"bank\" has different embeddings in different contexts\n",
        "\n",
        " BERT(\"I deposited money at the bank\") → \"bank\" = [0.4, 0.3, -0.2, ...]\n",
        "\n",
        " BERT(\"I sat by the river bank\") → \"bank\" = [0.1, -0.2, 0.5, ...]\n",
        "\n",
        "3. Sentence/Document-Level Embeddings\n",
        "\n",
        " * What they are: Represent entire sentences or documents as single vectors\n",
        " * Examples: Sentence-BERT, OpenAI embeddings, Universal Sentence Encoder\n",
        " * Advantages: Capture holistic meaning, efficient for retrieval\n",
        "\n",
        " OpenAI(\"Climate change is a global challenge\") = [0.02, -0.15, 0.08, ...]\n",
        "\n",
        "4. Bi-encoders vs. Cross-encoders\n",
        "\n",
        " * Bi-encoders: Encode query and document separately, then compare vectors (fast but less accurate)\n",
        " * Cross-encoders: Consider query and document together (more accurate but computationally expensive)\n"
      ],
      "metadata": {
        "id": "qWw9XeHRQC6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Model:** text-embedding-3-small\n",
        "\n",
        "* **Source:** OpenAI\n",
        "\n",
        "* **Best For:** General-purpose, high-quality, fast\n",
        "\n",
        "* **Dimensions:** 1536\n",
        "\n",
        "2. **Model:** all-MiniLM-L6-v2\n",
        "* **Source:** HuggingFace\n",
        "* **Best For:** Lightweight, quick search\n",
        "* **Dimensions:** 384\n",
        "\n",
        "3. **Model:** BAAI/bge-small-en\n",
        "* **Source:** HuggingFace\n",
        "* **Best For:** Great for English Q&A\n",
        "* **Dimensions:** 384\n",
        "\n",
        "4. **Model:** Cohere/embed-english-v3.0\n",
        "* **Source:** Cohere\n",
        "* **Best For:** Semantic search and classification\n",
        "* **Dimensions:** 1024\n",
        "\n",
        "5. **Model:** hkunlp/instructor-xl\n",
        "* **Source:** HuggingFace\n",
        "* **Best For:** Instruction-aware embedding\n",
        "* **Dimensions:** 768+"
      ],
      "metadata": {
        "id": "7P7scXB-RB5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **How to Select an Embedding Model**\n",
        "\n",
        "Consider these factors when choosing an embedding model:\n",
        "\n",
        "1. **Performance:** How well does it capture semantic similarity for your specific domain?\n",
        "2. **Speed:** How quickly can it generate embeddings? (Critical for real-time applications)\n",
        "3. **Cost:** API-based models like OpenAI have usage costs; open-source models have computing costs\n",
        "4. **Language support:** Does it work well for your target languages?\n",
        "5. **Dimensions:** Higher dimensions often capture more nuance but require more storage\n",
        "\n",
        "##### **Why Dimensions Matter**\n",
        "Vector dimensions represent the \"expressiveness\" of the embedding space:\n",
        "* Low dimensions (32-128): Faster, smaller storage, but less expressive\n",
        "* Medium dimensions (384-768): Good balance for many applications\n",
        "* High dimensions (1024+): More expressive, better for complex language understanding\n",
        "\n",
        "Think of dimensions as the \"vocabulary\" of the embedding space. More dimensions allow the model to capture more nuanced relationships, just as a larger vocabulary allows more precise expression in human language.\n"
      ],
      "metadata": {
        "id": "SGaAU_NoSrZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Practical Code to Generate Embeddings**"
      ],
      "metadata": {
        "id": "ITTpVB18Te1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "texts = [\"Regenerative farming helps soil\", \"How to use fertilizer safely?\"]\n",
        "embeddings = model.encode(texts)\n",
        "\n",
        "print(embeddings[0])  # Vector for first sentence"
      ],
      "metadata": {
        "id": "yeVPZqCLThni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Semantic Similarity in Code**"
      ],
      "metadata": {
        "id": "mrqKOS-nVMfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "query = \"Tips for healthy soil\"\n",
        "query_vec = model.encode([query])\n",
        "scores = cosine_similarity(query_vec, embeddings)\n",
        "\n",
        "most_similar = texts[scores[0].argmax()]"
      ],
      "metadata": {
        "id": "Ur6jNr25VG_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Common Mistakes with Embeddings in RAG Systems**\n",
        "\n",
        "1. **Using mismatched embeddings:** Ensure you use the SAME embedding model for both document indexing and queries\n",
        "2. **Ignoring language diversity:** Some embedding models perform poorly on non-English content or specialized domains\n",
        "3. **Overlooking chunking strategy:** The way you split documents affects embedding quality; chunks need sufficient context\n",
        "4. **Failing to normalize vectors:** Some distance calculations require normalized vectors for proper comparison\n",
        "5. **Using embedding models that don't match your use case:** Don't use a general-purpose model for highly specialized content\n",
        "6. **Ignoring dimensions:** Selecting models with too few dimensions can limit semantic expressiveness\n",
        "7. **Not considering retrieval method:** Different vector databases and retrieval algorithms may work better with specific embedding types"
      ],
      "metadata": {
        "id": "olx1RskhVYq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Why Embeddings Make or Break Your RAG System**\n",
        "\n",
        "The quality of embeddings directly affects retrieval performance:\n",
        "\n",
        "* Poor embeddings → Irrelevant documents retrieved → Inaccurate or hallucinated LLM responses\n",
        "* Good embeddings → Relevant documents retrieved → Factual, grounded LLM responses\n",
        "\n",
        "Think of embeddings as the \"eyes\" of your RAG system. If they can't properly see the semantic relationships between text, everything downstream will suffer.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Embeddings are the mathematical backbone that enables RAG systems to understand meaning and find relevant information. By transforming text into numerical vectors that preserve semantic relationships, embeddings allow computers to grasp relationships between concepts that would be impossible with simple keyword matching.\n",
        "\n",
        "As you build RAG applications, investing time in selecting the right embedding model and properly configuring your embedding pipeline will pay enormous dividends in system accuracy and user satisfaction.\n",
        "\n",
        "In our next session, we'll explore vector databases—specialized systems designed to efficiently store and search these embedding vectors at scale."
      ],
      "metadata": {
        "id": "5p8XW11PVzdD"
      }
    }
  ]
}
