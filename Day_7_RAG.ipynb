{
  "nbformat": 4, 
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNW57es2hkR6mHKCfoLWiMj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nareshedagotti/RAG/blob/main/Day_7_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Complete RAG Evaluation Guide: From Basics to Production**\n"
      ],
      "metadata": {
        "id": "K7GTD_y_MKR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **What is RAG Evaluation?**\n",
        "\n",
        "RAG (Retrieval-Augmented Generation) Evaluation is the systematic process of measuring and assessing the quality, accuracy, and effectiveness of RAG systems. It involves testing both the retrieval component (how well the system finds relevant information) and the generation component (how well the language model uses that information to create responses).\n",
        "\n",
        "---\n",
        "\n",
        "##### **Core Components of RAG Evaluation**\n",
        "\n",
        "**1. Retrieval Quality Assessment**\n",
        "- Measures how well the system retrieves relevant documents  \n",
        "- Evaluates ranking and relevance of retrieved content  \n",
        "- Assesses coverage of important information  \n",
        "\n",
        "**2. Generation Quality Assessment**\n",
        "- Measures how well the LLM uses retrieved information  \n",
        "- Evaluates accuracy, coherence, and relevance of generated responses  \n",
        "- Checks for hallucinations and factual errors  \n",
        "\n",
        "**3. End-to-End System Performance**\n",
        "- Measures overall user satisfaction  \n",
        "- Evaluates response time and system reliability  \n",
        "- Assesses real-world effectiveness  \n",
        "\n",
        "---\n",
        "\n",
        "##### **Why RAG Evaluation is Critical**\n",
        "\n",
        "**1. Quality Assurance**  \n",
        "RAG systems combine complex retrieval and generation processes. Without proper evaluation, you cannot ensure the system produces accurate, relevant, and reliable responses.\n",
        "\n",
        "**2. System Optimization**  \n",
        "Evaluation identifies bottlenecks and areas for improvement. Research shows that the retrieval component contributes to approximately 90% of RAG system quality.\n",
        "\n",
        "**3. Production Readiness**  \n",
        "Real-world deployment requires confidence in system performance across diverse queries and edge cases.\n",
        "\n",
        "**4. User Trust and Safety**  \n",
        "Especially critical in domains like healthcare, legal, or financial services where incorrect information can have serious consequences.\n",
        "\n",
        "**5. Regulatory Compliance**  \n",
        "Many industries require documented evaluation processes for AI systems.\n",
        "\n",
        "**6. Cost Optimization**  \n",
        "Proper evaluation helps optimize computational resources and reduce operational costs.\n",
        "\n",
        "---\n",
        "\n",
        "##### **Consequences of Not Evaluating RAG**\n",
        "\n",
        "**Immediate Consequences**\n",
        "\n",
        "**1. Silent Failures**  \n",
        "```python\n",
        "# Example: Dangerous misinformation goes undetected\n",
        "Query: \"What is the recommended dosage for aspirin?\"\n",
        "Bad Response: \"Take 10 tablets daily\"  # Potentially fatal\n",
        "# Without evaluation, this goes unnoticed until harm occurs\n",
        "```\n",
        "**2. Poor User Experience**\n",
        "\n",
        "- Irrelevant responses to queries  \n",
        "- Inconsistent quality across topics  \n",
        "- Slow response times  \n",
        "\n",
        "**3. Resource Waste**\n",
        "\n",
        "- Retrieving irrelevant documents  \n",
        "- Higher token usage due to poor ranking  \n",
        "- Inefficient system architecture  \n",
        "\n",
        "---\n",
        "\n",
        "##### **Long-term Consequences**\n",
        "\n",
        "**1. System Degradation**\n",
        "\n",
        "```python\n",
        "# Quality decline over time\n",
        "Month 1: Accuracy = 85%\n",
        "Month 6: Accuracy = 65%  # Unnoticed degradation\n",
        "Month 12: Accuracy = 45% # System becomes unreliable\n",
        "```\n",
        "**2. Business Impact**\n",
        "\n",
        "- Loss of user trust and engagement  \n",
        "- Increased support costs  \n",
        "- Legal liability in regulated industries  \n",
        "- Competitive disadvantage  \n",
        "\n",
        "**3. Technical Debt**\n",
        "\n",
        "- Difficult root cause identification  \n",
        "- Expensive fixes when discovered  \n",
        "- Increasingly fragile architecture  \n"
      ],
      "metadata": {
        "id": "HVshukIgMXC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **RAG Evaluation Types**\n",
        "##### **1. Retrieval Component Evaluation**\n",
        "\n",
        "##### **1.1 Context Precision**\n",
        "\n",
        "**What it is:** Measures whether relevant information appears early in retrieved results.\n",
        "\n",
        "**When to use:** When users need relevant information quickly (search engines, FAQs).\n",
        "\n",
        "**Formula:**\n",
        "Context Precision = (1/|GT|) × Σ(Precision@k × relevance@k)\n",
        "\n",
        "##### **Example Implementation:**\n"
      ],
      "metadata": {
        "id": "36GgAO5YNqfs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2d21Y_hMDRN"
      },
      "outputs": [],
      "source": [
        "def calculate_context_precision(retrieved_docs, ground_truth_docs):\n",
        "    precisions = []\n",
        "    relevant_count = 0\n",
        "\n",
        "    for i, doc in enumerate(retrieved_docs, 1):\n",
        "        if doc in ground_truth_docs:\n",
        "            relevant_count += 1\n",
        "            precision_at_i = relevant_count / i\n",
        "            precisions.append(precision_at_i)\n",
        "\n",
        "    gt_positions = [i for i, doc in enumerate(retrieved_docs)\n",
        "                   if doc in ground_truth_docs]\n",
        "\n",
        "    if not gt_positions:\n",
        "        return 0.0\n",
        "\n",
        "    relevant_precisions = [precisions[i-1] for i in gt_positions]\n",
        "    return sum(relevant_precisions) / len(ground_truth_docs)\n",
        "\n",
        "# Example usage\n",
        "retrieved = [\"doc1\", \"doc2\", \"doc3\", \"doc4\", \"doc5\"]\n",
        "ground_truth = {\"doc2\", \"doc3\", \"doc5\"}\n",
        "precision = calculate_context_precision(retrieved, ground_truth)\n",
        "print(f\"Context Precision: {precision:.3f}\")  # Output: 0.622"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Cases:**\n",
        "* E-commerce product search\n",
        "* Document retrieval systems\n",
        "* Knowledge base systems\n"
      ],
      "metadata": {
        "id": "x0-RlClgN_xC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **1.2 Context Recall**\n",
        "\n",
        "**What it is:** Measures what proportion of relevant information is captured in retrieved context.\n",
        "\n",
        "**When to use:** When completeness is critical (medical diagnosis, legal research).\n",
        "\n",
        "**Formula:**\n",
        "Context Recall = Found GT claims / Total GT claims\n",
        "\n",
        "##### **Example Implementation:**\n"
      ],
      "metadata": {
        "id": "5eIM-icIOIYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_context_recall(retrieved_context, ground_truth_claims):\n",
        "    found_claims = 0\n",
        "    context_text = \" \".join(retrieved_context)\n",
        "\n",
        "    for claim in ground_truth_claims:\n",
        "        if can_infer_claim(claim, context_text):\n",
        "            found_claims += 1\n",
        "\n",
        "    return found_claims / len(ground_truth_claims) if ground_truth_claims else 0\n",
        "\n",
        "def can_infer_claim(claim, context):\n",
        "    # Simple keyword-based approach (use NLI models in production)\n",
        "    claim_keywords = extract_key_terms(claim)\n",
        "    context_lower = context.lower()\n",
        "\n",
        "    found_keywords = sum(1 for keyword in claim_keywords\n",
        "                        if keyword.lower() in context_lower)\n",
        "\n",
        "    return (found_keywords / len(claim_keywords)) >= 0.7\n",
        "\n",
        "# Example usage\n",
        "retrieved_context = [\n",
        "    \"Water has the chemical formula H2O and is essential for life.\",\n",
        "    \"It covers approximately 71% of Earth's surface.\"\n",
        "]\n",
        "\n",
        "ground_truth_claims = [\n",
        "    \"Water has chemical formula H2O\",\n",
        "    \"Water boils at 100°C at sea level\",\n",
        "    \"Water is essential for life\"\n",
        "]\n",
        "\n",
        "recall = calculate_context_recall(retrieved_context, ground_truth_claims)\n",
        "print(f\"Context Recall: {recall:.3f}\")  # Output: 0.667"
      ],
      "metadata": {
        "id": "Faa98u_dN_YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Cases:**\n",
        "* Medical diagnosis systems\n",
        "* Legal research platforms\n",
        "* Scientific literature review\n"
      ],
      "metadata": {
        "id": "BmbwyJHCOWiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **1.3 Mean Reciprocal Rank (MRR)**\n",
        "**What it is:** Measures average reciprocal rank of first relevant document.\n",
        "\n",
        "**When to use:** When finding at least one highly relevant result quickly matters.\n",
        "\n",
        "**Formula:**\n",
        "MRR = (1/|Q|) × Σ(1/rank_i)\n",
        "\n",
        "##### **Example Implementation:**\n"
      ],
      "metadata": {
        "id": "Bd7Mz8bBOdYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mrr(query_results):\n",
        "    reciprocal_ranks = []\n",
        "\n",
        "    for query, retrieved, relevant in query_results:\n",
        "        first_relevant_rank = None\n",
        "\n",
        "        for i, doc in enumerate(retrieved, 1):\n",
        "            if doc in relevant:\n",
        "                first_relevant_rank = i\n",
        "                break\n",
        "\n",
        "        if first_relevant_rank:\n",
        "            reciprocal_ranks.append(1.0 / first_relevant_rank)\n",
        "        else:\n",
        "            reciprocal_ranks.append(0.0)\n",
        "\n",
        "    return sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
        "\n",
        "# Example usage\n",
        "query_results = [\n",
        "    (\"python tutorial\", [\"doc1\", \"doc2\", \"doc3\"], {\"doc2\"}),  # RR = 1/2\n",
        "    (\"machine learning\", [\"doc5\", \"doc6\", \"doc7\"], {\"doc5\"}), # RR = 1/1\n",
        "]\n",
        "\n",
        "mrr_score = calculate_mrr(query_results)\n",
        "print(f\"MRR: {mrr_score:.3f}\")  # Output: 0.750"
      ],
      "metadata": {
        "id": "To7Inbb9MJvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Cases:**\n",
        "* Web search engines\n",
        "* Question answering systems\n",
        "* Product recommendations\n"
      ],
      "metadata": {
        "id": "NkfwGbZGOuwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Generation Component Evaluation**\n",
        "\n"
      ],
      "metadata": {
        "id": "S_VZPjIRO9kI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.1 Faithfulness**\n",
        "\n",
        "**What it is:**Measures how well generated answers stay true to provided context.\n",
        "\n",
        "**When to use:** Critical for accuracy-dependent applications (medical, financial).\n",
        "\n",
        "**Formula:**\n",
        "Faithfulness = Verifiable claims / Total claims in answer\n",
        "\n",
        "##### **Example Implementation:**"
      ],
      "metadata": {
        "id": "IGVqDHlhO9ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "\n",
        "class FaithfulnessEvaluator:\n",
        "    def __init__(self):\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.threshold = 0.7\n",
        "\n",
        "    def extract_claims(self, text):\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        claims = [s.strip() for s in sentences\n",
        "                 if len(s.strip()) > 10 and not s.endswith('?')]\n",
        "        return claims\n",
        "\n",
        "    def can_infer_claim(self, claim, context):\n",
        "        claim_embedding = self.model.encode([claim])\n",
        "        context_sentences = re.split(r'[.!?]+', context)\n",
        "        context_sentences = [s.strip() for s in context_sentences if len(s.strip()) > 10]\n",
        "\n",
        "        if not context_sentences:\n",
        "            return False\n",
        "\n",
        "        context_embeddings = self.model.encode(context_sentences)\n",
        "        similarities = cosine_similarity(claim_embedding, context_embeddings)[0]\n",
        "\n",
        "        return max(similarities) >= self.threshold\n",
        "\n",
        "    def calculate_faithfulness(self, answer, context):\n",
        "        claims = self.extract_claims(answer)\n",
        "        if not claims:\n",
        "            return 1.0\n",
        "\n",
        "        faithful_claims = sum(1 for claim in claims\n",
        "                            if self.can_infer_claim(claim, context))\n",
        "\n",
        "        return faithful_claims / len(claims)\n",
        "\n",
        "# Example usage\n",
        "evaluator = FaithfulnessEvaluator()\n",
        "\n",
        "context = \"Napoleon was defeated at Waterloo on June 18, 1815.\"\n",
        "answer = \"Napoleon lost the Battle of Waterloo on June 18, 1815.\"\n",
        "\n",
        "faithfulness = evaluator.calculate_faithfulness(answer, context)\n",
        "print(f\"Faithfulness: {faithfulness:.3f}\")  # Output: 1.000"
      ],
      "metadata": {
        "id": "cfSEtHprMJ0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Cases:**\n",
        "* Medical information systems\n",
        "* Financial advisory platforms\n",
        "* Legal research tools"
      ],
      "metadata": {
        "id": "zvmxoBYXPUWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.2 Answer Relevance**\n",
        "\n",
        "**What it is:** Measures how well answers address original questions.\n",
        "\n",
        "**When to use:** To ensure responses stay on-topic and directly address queries.\n",
        "\n",
        "**Formula:**\n",
        "Answer Relevance = Average cosine similarity between original and generated questions\n",
        "\n",
        "##### **Example Implementation:**"
      ],
      "metadata": {
        "id": "xNUK7ixgPZjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnswerRelevanceEvaluator:\n",
        "    def __init__(self):\n",
        "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def calculate_answer_relevance(self, original_question, answer):\n",
        "        # Generate questions from answer (simplified version)\n",
        "        generated_questions = self.generate_questions_from_answer(answer)\n",
        "\n",
        "        if not generated_questions:\n",
        "            return 0.0\n",
        "\n",
        "        original_embedding = self.encoder.encode([original_question])\n",
        "        generated_embeddings = self.encoder.encode(generated_questions)\n",
        "\n",
        "        similarities = []\n",
        "        for gen_embedding in generated_embeddings:\n",
        "            similarity = cosine_similarity(\n",
        "                original_embedding.reshape(1, -1),\n",
        "                gen_embedding.reshape(1, -1)\n",
        "            )[0][0]\n",
        "            similarities.append(similarity)\n",
        "\n",
        "        return np.mean(similarities)\n",
        "\n",
        "    def generate_questions_from_answer(self, answer):\n",
        "        # Simplified question generation\n",
        "        # In production, use LLM-based generation\n",
        "        return [f\"What is {answer[:50]}?\"]\n",
        "\n",
        "# Example usage\n",
        "evaluator = AnswerRelevanceEvaluator()\n",
        "relevance = evaluator.calculate_answer_relevance(\n",
        "    \"What is water's formula?\",\n",
        "    \"Water has the chemical formula H2O\"\n",
        ")\n",
        "print(f\"Answer Relevance: {relevance:.3f}\")"
      ],
      "metadata": {
        "id": "QXbev_GYMJ4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Cases:\n",
        "* Customer service chatbots\n",
        "* Educational Q&A systems\n",
        "* Content recommendation"
      ],
      "metadata": {
        "id": "HupWLedyPf-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.3 Answer Semantic Similarity**\n",
        "\n",
        "**What it is:** Measures semantic similarity between generated and reference answers.\n",
        "\n",
        "**When to use:** When you have gold standard answers for comparison.\n",
        "\n",
        "**Formula:**\n",
        "Semantic Similarity = cosine_similarity(generated_embedding, reference_embedding)\n",
        "\n",
        "##### **Example Implementation:**\n"
      ],
      "metadata": {
        "id": "IfrVW6H4QEoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SemanticSimilarityEvaluator:\n",
        "    def __init__(self):\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def calculate_similarity(self, generated_answer, reference_answer):\n",
        "        if not generated_answer.strip() or not reference_answer.strip():\n",
        "            return 0.0\n",
        "\n",
        "        generated_embedding = self.model.encode([generated_answer])\n",
        "        reference_embedding = self.model.encode([reference_answer])\n",
        "\n",
        "        similarity = cosine_similarity(generated_embedding, reference_embedding)[0][0]\n",
        "        return float(similarity)\n",
        "\n",
        "# Example usage\n",
        "evaluator = SemanticSimilarityEvaluator()\n",
        "similarity = evaluator.calculate_similarity(\n",
        "    \"H2O is water's chemical formula\",\n",
        "    \"Water has the chemical formula H2O\"\n",
        ")\n",
        "print(f\"Semantic Similarity: {similarity:.3f}\")  # Output: ~0.85"
      ],
      "metadata": {
        "id": "YEdoWYqlPgaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Cases:**\n",
        "* Educational assessment\n",
        "* Content quality control\n",
        "* Translation evaluation"
      ],
      "metadata": {
        "id": "s4bMyZDMQHXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. End-to-End Evaluation**"
      ],
      "metadata": {
        "id": "IfG83qBhQwHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.1 User Satisfaction**\n",
        "\n",
        "**What it is:** Direct measurement of user satisfaction with responses.\n",
        "\n",
        "**When to use:** For production systems where user experience is paramount.\n",
        "\n",
        "**Example Implementation:**\n"
      ],
      "metadata": {
        "id": "6EV4ycs-Q0-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UserSatisfactionTracker:\n",
        "    def __init__(self):\n",
        "        self.feedback_db = []\n",
        "\n",
        "    def collect_feedback(self, query, answer, rating, feedback_text=None):\n",
        "        feedback = {\n",
        "            'query': query,\n",
        "            'answer': answer,\n",
        "            'rating': rating,  # 1-5 scale\n",
        "            'feedback_text': feedback_text,\n",
        "            'timestamp': datetime.now()\n",
        "        }\n",
        "        self.feedback_db.append(feedback)\n",
        "\n",
        "    def calculate_metrics(self):\n",
        "        ratings = [f['rating'] for f in self.feedback_db]\n",
        "        return {\n",
        "            'average_rating': np.mean(ratings),\n",
        "            'satisfaction_rate': sum(1 for r in ratings if r >= 4) / len(ratings),\n",
        "            'total_responses': len(ratings)\n",
        "        }"
      ],
      "metadata": {
        "id": "pwJLe4fnQ021"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.2 Response Time**\n",
        "\n",
        "**What it is:** Time measurement for retrieval and generation.\n",
        "\n",
        "**When to use:** When response speed affects user experience.\n",
        "\n",
        "**Example Implementation:**\n"
      ],
      "metadata": {
        "id": "L8yKWPEKRAY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResponseTimeEvaluator:\n",
        "    def measure_response_time(self, rag_system, query):\n",
        "        start_time = time.time()\n",
        "\n",
        "        retrieval_start = time.time()\n",
        "        context = rag_system.retrieve(query)\n",
        "        retrieval_time = time.time() - retrieval_start\n",
        "\n",
        "        generation_start = time.time()\n",
        "        answer = rag_system.generate(query, context)\n",
        "        generation_time = time.time() - generation_start\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            'total_time': total_time,\n",
        "            'retrieval_time': retrieval_time,\n",
        "            'generation_time': generation_time\n",
        "        }"
      ],
      "metadata": {
        "id": "Hi38V5ydRAGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hallucination in RAG Systems**\n",
        "\n",
        "---\n",
        "\n",
        "##### **What is Hallucination?**  \n",
        "Hallucination occurs when RAG systems generate information that is not supported by the retrieved context or factual knowledge.\n",
        "\n",
        "---\n",
        "\n",
        "##### **Why Check for Hallucinations?**  \n",
        "- **Safety**: Prevents harmful misinformation  \n",
        "- **Trust**: Maintains user confidence  \n",
        "- **Compliance**: Meets regulatory requirements  \n",
        "- **Quality**: Ensures system reliability  \n",
        "\n",
        "---\n",
        "\n",
        "##### **Types of Hallucinations**\n",
        "\n",
        "---\n",
        "\n",
        "**1. Factual Hallucinations**  \n",
        "Generating false facts not in the context.  \n",
        "\n",
        "```python\n",
        "# Example\n",
        "Context: \"Paris is the capital of France\"\n",
        "Query: \"What is the population of Paris?\"\n",
        "Hallucinated Answer: \"Paris has 15 million people\"  # Not in context\n",
        "```\n",
        "**2. Contextual Hallucinations**  \n",
        "Information not present in retrieved context.\n",
        "\n",
        "```python\n",
        "# Example\n",
        "Context: \"Water boils at 100°C\"\n",
        "Query: \"What is water's freezing point?\"\n",
        "Hallucinated Answer: \"Water freezes at 0°C\"  # Not in provided context\n",
        "```\n",
        "##### **3. Logical Hallucinations**  \n",
        "Contradictory or illogical reasoning.\n",
        "\n",
        "```python\n",
        "# Example\n",
        "Context: \"The meeting is scheduled for Monday\"\n",
        "Query: \"When is the meeting?\"\n",
        "Hallucinated Answer: \"The meeting is on Tuesday\"  # Contradicts context\n",
        "```\n"
      ],
      "metadata": {
        "id": "SL5QNjqgRNN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **How to Check for Hallucinations**\n",
        "**1. Claim-Level Verification**\n"
      ],
      "metadata": {
        "id": "0jd9S4ocR22M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_hallucinations(answer, context):\n",
        "    claims = extract_claims(answer)\n",
        "    hallucinations = []\n",
        "\n",
        "    for claim in claims:\n",
        "        if not can_be_inferred(claim, context):\n",
        "            hallucinations.append(claim)\n",
        "\n",
        "    return len(hallucinations) / len(claims) if claims else 0\n",
        "\n",
        "# Example usage\n",
        "context = \"Water boils at 100°C at sea level\"\n",
        "answer = \"Water boils at 90°C and freezes at -5°C\"\n",
        "hallucination_rate = detect_hallucinations(answer, context)\n",
        "print(f\"Hallucination Rate: {hallucination_rate:.3f}\")"
      ],
      "metadata": {
        "id": "292x9sDIRNwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Consistency Checking**"
      ],
      "metadata": {
        "id": "CYRkMaU9SD32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_consistency(rag_system, query, n_runs=3):\n",
        "    answers = [rag_system.generate_answer(query) for _ in range(n_runs)]\n",
        "\n",
        "    # Check for contradictions between answers\n",
        "    contradictions = 0\n",
        "    for i in range(len(answers)):\n",
        "        for j in range(i+1, len(answers)):\n",
        "            if are_contradictory(answers[i], answers[j]):\n",
        "                contradictions += 1\n",
        "\n",
        "    total_pairs = len(answers) * (len(answers) - 1) / 2\n",
        "    consistency_score = 1 - (contradictions / total_pairs)\n",
        "    return consistency_score"
      ],
      "metadata": {
        "id": "UDb1sR3DR9_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. External Knowledge Validation**"
      ],
      "metadata": {
        "id": "3-9NJZQNSLlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_against_knowledge_base(answer, knowledge_base):\n",
        "    claims = extract_claims(answer)\n",
        "    validated_claims = 0\n",
        "\n",
        "    for claim in claims:\n",
        "        if is_supported_by_kb(claim, knowledge_base):\n",
        "            validated_claims += 1\n",
        "\n",
        "    return validated_claims / len(claims) if claims else 1.0"
      ],
      "metadata": {
        "id": "zg8ij6HkR-C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hallucination Detection Implementation**"
      ],
      "metadata": {
        "id": "9GO1IsHbSPyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HallucinationDetector:\n",
        "    def __init__(self):\n",
        "        self.fact_checker = FactChecker()\n",
        "        self.consistency_checker = ConsistencyChecker()\n",
        "\n",
        "    def detect_hallucinations(self, query, answer, context):\n",
        "        results = {\n",
        "            'factual_accuracy': self.check_factual_accuracy(answer, context),\n",
        "            'contextual_grounding': self.check_contextual_grounding(answer, context),\n",
        "            'logical_consistency': self.check_logical_consistency(answer),\n",
        "            'external_validation': self.validate_externally(answer)\n",
        "        }\n",
        "\n",
        "        # Combine scores\n",
        "        overall_score = np.mean(list(results.values()))\n",
        "        results['overall_hallucination_score'] = 1 - overall_score\n",
        "\n",
        "        return results\n",
        "\n",
        "    def check_factual_accuracy(self, answer, context):\n",
        "        # Implementation for factual checking\n",
        "        pass\n",
        "\n",
        "    def check_contextual_grounding(self, answer, context):\n",
        "        # Implementation for context grounding\n",
        "        pass"
      ],
      "metadata": {
        "id": "xi7-r5uZSUVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Common Production Failures in RAG Systems**\n",
        "\n",
        "---\n",
        "\n",
        "##### **1. Context Length Limitations**  \n",
        "**Problem:** Retrieved context exceeds model token limits, causing truncation or processing failures.  \n",
        "**Real-world Example:** A legal document search system retrieves 10 relevant case studies totaling 50,000 tokens, but the model only accepts 32,000 tokens, resulting in important context being cut off mid-sentence.  \n",
        "**Solutions:**\n",
        "- **Intelligent Context Ranking:** Prioritize retrieved chunks based on semantic similarity to the query  \n",
        "- **Hierarchical Summarization:** Create multi-level summaries of lengthy documents  \n",
        "- **Context Windowing:** Use overlapping context windows to maintain coherence  \n",
        "- **Dynamic Truncation:** Cut at natural break points (sentence/paragraph boundaries) rather than arbitrary token limits  \n",
        "\n",
        "---\n",
        "\n",
        "##### **2. Retrieval Quality Degradation**  \n",
        "**Problem:** Gradual decline in retrieval accuracy over time due to index corruption, embedding drift, or changing data patterns.  \n",
        "**Real-world Example:** A customer support RAG system starts returning increasingly irrelevant articles over months, causing customer satisfaction scores to drop from 85% to 60%.  \n",
        "**Solutions:**\n",
        "- **Automated Quality Benchmarks:** Run daily tests against golden question-answer pairs  \n",
        "- **Retrieval Metrics Monitoring:** Track precision@k, recall@k, and NDCG scores  \n",
        "- **A/B Testing Framework:** Compare current performance against baseline models  \n",
        "- **Periodic Reindexing:** Schedule regular complete index rebuilds  \n",
        "- **Embedding Model Updates:** Monitor for newer, better-performing embedding models  \n",
        "\n",
        "---\n",
        "\n",
        "##### **3. Hallucination in Production**  \n",
        "**Problem:** Model generates plausible but false information not supported by retrieved context.  \n",
        "**Real-world Example:** A medical information RAG system confidently states \"Drug X is approved for treating condition Y\" when the retrieved context only mentions ongoing clinical trials.  \n",
        "**Solutions:**\n",
        "- **Multi-Generation Validation:** Generate multiple answers and check for consistency  \n",
        "- **Context Grounding Checks:** Verify each claim can be traced back to source material  \n",
        "- **Confidence Scoring:** Return uncertainty indicators with answers  \n",
        "- **Fallback Responses:** Use \"I don't know\" templates when confidence is low  \n",
        "- **External Fact Verification:** Cross-reference critical claims with authoritative sources  \n",
        "\n",
        "---\n",
        "\n",
        "##### **4. Performance Bottlenecks**  \n",
        "**Problem:** Slow response times affecting user experience, especially during peak usage.  \n",
        "**Real-world Example:** An e-commerce product recommendation system takes 8–12 seconds to respond during Black Friday traffic, causing 40% of users to abandon their searches.  \n",
        "**Solutions:**\n",
        "- **Multi-Layer Caching:** Cache frequent queries, embeddings, and intermediate results  \n",
        "- **Async Processing:** Use background workers for non-critical operations  \n",
        "- **Vector Database Optimization:** Implement approximate nearest neighbor search  \n",
        "- **Load Balancing:** Distribute queries across multiple model instances  \n",
        "- **Pre-computation:** Generate answers for common questions in advance  \n",
        "- **Response Streaming:** Send partial results while processing continues  \n",
        "\n",
        "---\n",
        "\n",
        "##### **5. Data Drift and Knowledge Staleness**  \n",
        "**Problem:** Knowledge base becomes outdated, leading to incorrect or obsolete information.  \n",
        "**Real-world Example:** A financial advisory RAG system continues recommending investment strategies based on pre-pandemic market conditions, missing crucial economic changes.  \n",
        "**Solutions:**\n",
        "- **Content Freshness Tracking:** Monitor document ages and update frequencies  \n",
        "- **Automated Source Monitoring:** Check original sources for changes  \n",
        "- **Version Control:** Maintain document versioning and change logs  \n",
        "- **Incremental Updates:** Add new information without full reindexing  \n",
        "- **Deprecation Workflows:** Mark and phase out outdated content  \n",
        "- **Real-time Data Integration:** Connect to live data feeds for dynamic information  \n",
        "\n",
        "---\n",
        "\n",
        "##### **6. Security and Privacy Violations**  \n",
        "**Problem:** Sensitive information leakage through retrieved context or generated responses.  \n",
        "**Real-world Example:** A HR chatbot accidentally reveals salary information from one employee's query to another employee asking about company policies.  \n",
        "**Solutions:**\n",
        "- **Access Control Integration:** Implement user-based document filtering  \n",
        "- **PII Detection and Masking:** Automatically redact sensitive information  \n",
        "- **Audit Logging:** Track all queries and responses for compliance  \n",
        "- **Data Classification:** Tag documents with sensitivity levels  \n",
        "- **Response Sanitization:** Remove potentially sensitive information from outputs  \n",
        "\n",
        "---\n",
        "\n",
        "##### **7. Inconsistent Answer Quality**  \n",
        "**Problem:** Wide variation in answer quality across different topics or query types.  \n",
        "**Real-world Example:** A technical documentation RAG system excels at API questions but performs poorly on conceptual architecture queries, creating user confusion about system reliability.  \n",
        "**Solutions:**\n",
        "- **Domain-Specific Models:** Use specialized models for different content types  \n",
        "- **Quality Scoring Frameworks:** Implement consistent evaluation metrics  \n",
        "- **Feedback Loop Integration:** Learn from user ratings and corrections  \n",
        "- **Content Gap Analysis:** Identify and fill knowledge base weaknesses  \n",
        "- **Answer Template Systems:** Provide structured response formats for consistency  \n"
      ],
      "metadata": {
        "id": "ky0BJwp8Skmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Evaluation Tools & Frameworks**\n",
        "\n",
        "##### **1. RAGAS (RAG Assessment)**\n",
        "\n",
        "**Purpose**: Comprehensive RAG evaluation framework\n",
        "\n",
        "**Implementation:**"
      ],
      "metadata": {
        "id": "7LXYpiHJSo2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
        "\n",
        "# Evaluate your RAG system\n",
        "result = evaluate(\n",
        "    dataset=your_test_dataset,\n",
        "    metrics=[faithfulness, answer_relevancy, context_precision]\n",
        ")\n",
        "\n",
        "print(f\"Faithfulness: {result['faithfulness']:.3f}\")\n",
        "print(f\"Answer Relevancy: {result['answer_relevancy']:.3f}\")"
      ],
      "metadata": {
        "id": "YGRLvvfgSVyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Features:**\n",
        "* Multiple evaluation metrics\n",
        "* Automated assessment\n",
        "* Integration with popular RAG frameworks\n"
      ],
      "metadata": {
        "id": "-c6NsH_aS5bV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2. DeepEval**\n",
        "\n",
        "**Purpose:** Modular evaluation framework for LLM applications\n",
        "\n",
        "**Implementation:**\n"
      ],
      "metadata": {
        "id": "rsBb4PO3S98Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval import evaluate\n",
        "from deepeval.metrics import FaithfulnessMetric, AnswerRelevancyMetric\n",
        "\n",
        "faithfulness_metric = FaithfulnessMetric(threshold=0.8)\n",
        "relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\n",
        "\n",
        "evaluate(\n",
        "    test_cases=[test_case],\n",
        "    metrics=[faithfulness_metric, relevancy_metric]\n",
        ")"
      ],
      "metadata": {
        "id": "6OcvoJXdS-R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Features:**\n",
        "* Customizable metrics\n",
        "* Real-time evaluation\n",
        "* Detailed reporting"
      ],
      "metadata": {
        "id": "pr7NjD_UTH9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3. TruLens**\n",
        "\n",
        "**Purpose:** Real-time evaluation and monitoring for LLM applications\n",
        "\n",
        "**Implementation:**\n"
      ],
      "metadata": {
        "id": "_HRMRd6_TLDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trulens_eval import TruLlama\n",
        "\n",
        "# Wrap your RAG app\n",
        "tru_rag = TruLlama(your_rag_app)\n",
        "\n",
        "# Automatic logging and evaluation\n",
        "with tru_rag as recording:\n",
        "    response = your_rag_app.query(\"Your question\")"
      ],
      "metadata": {
        "id": "UApR1y2VTK7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Features:**\n",
        "* Production monitoring\n",
        "* Interactive dashboards\n",
        "* Feedback collection"
      ],
      "metadata": {
        "id": "EfRxO4hBTVg7"
      }
    }
  ]
}
